{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Starting Stage 1: Object Detection and Cropping...\n",
      ". Processing malawi-cyclone_00000034_pre_disaster.jpg. Processing malawi-cyclone_00000066_pre_disaster.jpg. Processing malawi-cyclone_00000212_pre_disaster.jpg\n",
      "[INFO] - Stage 1 finished. Saving crop data...\n",
      "[INFO] - Crop data saved to data\\data_pkl_path.pkl\n",
      "\n",
      "[INFO] - Starting Stage 2: Classification and Plotting...\n",
      "[INFO] - Loaded NDVI data from data\\test_ndvi.csv\n",
      "[INFO] - Using device: cpu\n",
      "[INFO] - Loaded classifier weights from data\\classifier_weights/efficientvit_b0.r224_in1k_3_patch_model_fold_0.pth\n",
      "[INFO] - Loading data from data\\data_pkl_path.pkl\n",
      "[INFO] - Processing & Plotting: malawi-cyclone_00000034\n",
      "[INFO] - Processing & Plotting: malawi-cyclone_00000066\n",
      "[INFO] - Processing & Plotting: malawi-cyclone_00000212\n",
      "\n",
      "[INFO] - Generating final submission file...\n",
      "[INFO] - Submission file saved to data\\sample_inference_output.csv\n",
      "[INFO] - Total 'destroyed' count across all images: 2\n",
      "[INFO] - Script finished.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO, RTDETR\n",
    "import os\n",
    "import pdb\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from ensemble_boxes import *\n",
    "import torch # Added\n",
    "import torch.nn as nn # Added\n",
    "import albumentations as A # Added\n",
    "from albumentations.pytorch.transforms import ToTensorV2 # Added\n",
    "from timm import create_model # Added\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "os.environ[\"ALBUMENTATIONS_DISABLE_VERSION_CHECK\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "AUGMENT = True\n",
    "RUN = 19\n",
    "RUN2 = 19\n",
    "loc = f'runs/detect/train{RUN}'\n",
    "loc2 = f'runs/detect/train{RUN2}'\n",
    "SZ = (928, 448)\n",
    "SZ2 = (928, 448) # 928\n",
    "\n",
    "THRESH = 0.5\n",
    "IOU_THRESH = 0.2\n",
    "CONF = 0.5 # 0.5\n",
    "YOLO_IOU_THR = 0.35\n",
    "EXT = 'jpg'\n",
    "\n",
    "# Model paths\n",
    "# Using the same model for both paths as per original code\n",
    "model_path = \"data/yolo_weights/best.pt\"\n",
    "model_path2 = \"data/yolo_weights/best.pt\"\n",
    "\n",
    "# Directories\n",
    "base_dir = \"data\"\n",
    "test_dir = os.path.join(base_dir, \"sample_jpg_images\") # .jpg images\n",
    "tif_dir = os.path.join(base_dir, \"test_images/Images\") # .tif images (assuming same dir based on original code)\n",
    "crops_dir = os.path.join(base_dir, \"cropped_images\")\n",
    "annotated_dir = os.path.join(base_dir, \"annotated_images\") # Directory for output plots\n",
    "data_pkl_path = os.path.join(base_dir, \"data_pkl_path.pkl\")#'/kaggle/working/data_with_bboxes.pkl' # Save bboxes here\n",
    "submission_csv_path = os.path.join(base_dir, \"sample_inference_output.csv\") # Output submission file path\n",
    "classifier_model_path = os.path.join(base_dir, \"classifier_weights/efficientvit_b0.r224_in1k_3_patch_model_fold_0.pth\")\n",
    "ndvi_csv_path = os.path.join(base_dir, \"test_ndvi.csv\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(crops_dir, exist_ok=True)\n",
    "os.makedirs(annotated_dir, exist_ok=True)\n",
    "\n",
    "# --- Helper Functions (from original code) ---\n",
    "def get_areas(boxes):\n",
    "    areas = []\n",
    "    for b in boxes:\n",
    "        arr = abs(b[2] - b[0]) * abs(b[3] - b[1])\n",
    "        areas.append(arr)\n",
    "    return areas\n",
    "\n",
    "def get_results(res, threshold=0.5):\n",
    "    res     = res.cpu().numpy()\n",
    "    cls     = res.boxes.cls\n",
    "    conf    = res.boxes.conf\n",
    "    boxes   = res.boxes.xyxyn # Keep normalized coordinates for WBF\n",
    "    mask    = conf > threshold\n",
    "    conf = conf[mask]\n",
    "    cls = cls[mask]\n",
    "    boxes = boxes[mask]\n",
    "    return cls, boxes, conf\n",
    "\n",
    "# WBF function (using weighted_boxes_fusion from original code)\n",
    "def run_wbf(bboxes, confs,labels, image_size_w, image_size_h, iou_thr=0.50, skip_box_thr=0.0001, weights=None):\n",
    "    # boxes need to be normalized for WBF\n",
    "    boxes =  [bbox for bbox in bboxes]\n",
    "    scores = [conf for conf in confs]\n",
    "    labels_list = [lab for lab in labels] # Assuming labels are already in correct format\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    # Convert boxes back to absolute coordinates\n",
    "    boxes[:, 0] = boxes[:, 0] * image_size_w\n",
    "    boxes[:, 1] = boxes[:, 1] * image_size_h\n",
    "    boxes[:, 2] = boxes[:, 2] * image_size_w\n",
    "    boxes[:, 3] = boxes[:, 3] * image_size_h\n",
    "    return boxes, scores, labels\n",
    "\n",
    "\n",
    "# --- Stage 1: Object Detection and Cropping ---\n",
    "print(\"[INFO] - Starting Stage 1: Object Detection and Cropping...\")\n",
    "imgs = sorted([os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith(f'_pre_disaster.{EXT}')])\n",
    "# tifs = [os.path.join(tif_dir, f) for f in os.listdir(tif_dir) if f.endswith(f'_pre_disaster.{EXT}')] # Not used directly later\n",
    "\n",
    "model = YOLO(model_path)\n",
    "model2 = YOLO(model_path2) # Using the same model weights again\n",
    "\n",
    "data = {} # Dictionary to store crop info and bboxes\n",
    "\n",
    "for im_pre_path in imgs:\n",
    "    print(f\". Processing {os.path.basename(im_pre_path)}\", end=\"\", flush=True)\n",
    "    im_post_path = im_pre_path.replace('_pre_disaster', '_post_disaster')\n",
    "    image_id = os.path.basename(im_pre_path).replace(f'_pre_disaster.{EXT}', '')\n",
    "\n",
    "    # Check if post image exists\n",
    "    if not os.path.exists(im_post_path):\n",
    "        print(f\"\\nWarning: Post image not found for {image_id}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    data[image_id] = {'pre_crops': [], 'post_crops': [], 'bboxes': []} # Initialize structure\n",
    "\n",
    "    try:\n",
    "        # Read images AFTER checking existence\n",
    "        img_pre = cv2.imread(im_pre_path)\n",
    "        img_post = cv2.imread(im_post_path)\n",
    "\n",
    "        if img_pre is None or img_post is None:\n",
    "             print(f\"\\nWarning: Could not read pre or post image for {image_id}, skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Run inference on pre-disaster image\n",
    "        results_pre = model(im_pre_path, imgsz=SZ, augment=AUGMENT, conf=CONF, iou=YOLO_IOU_THR, verbose=False)\n",
    "        results_pre2 = model2(im_pre_path, imgsz=SZ2, augment=AUGMENT, conf=CONF, iou=YOLO_IOU_THR, verbose=False)\n",
    "\n",
    "        if not results_pre or not results_pre2:\n",
    "             print(f\"\\nWarning: No results from inference for {image_id}, skipping.\")\n",
    "             continue\n",
    "\n",
    "        res_pre = results_pre[0]\n",
    "        res_pre2 = results_pre2[0]\n",
    "        orig_h, orig_w = res_pre.orig_shape # Get original dimensions\n",
    "\n",
    "        ccls_pre, bbxs_pre_norm, cnfs_pre = get_results(res_pre, threshold=THRESH)\n",
    "        ccls_pre2, bbxs_pre2_norm, cnfs_pre2 = get_results(res_pre2, threshold=THRESH)\n",
    "\n",
    "        # Run Weighted Boxes Fusion (WBF) - Pass original dimensions\n",
    "        boxes_abs, scores_pre, labels_pre = run_wbf(\n",
    "                                         [bbxs_pre_norm, bbxs_pre2_norm],\n",
    "                                         [cnfs_pre, cnfs_pre2],\n",
    "                                         [ccls_pre, ccls_pre2], # Pass class labels\n",
    "                                         orig_w, orig_h, # Pass original W and H\n",
    "                                         iou_thr=IOU_THRESH\n",
    "                                         )\n",
    "\n",
    "        # Crop and save patches based on WBF results\n",
    "        for i in range(len(boxes_abs)):\n",
    "            x1, y1, x2, y2 = map(int, boxes_abs[i]) # Ensure integer coordinates\n",
    "\n",
    "            # Clamp coordinates to image boundaries to avoid errors\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(orig_w, x2)\n",
    "            y2 = min(orig_h, y2)\n",
    "\n",
    "            # Ensure valid box dimensions\n",
    "            if x1 >= x2 or y1 >= y2:\n",
    "                #print(f\"\\nWarning: Invalid box dimensions for {image_id} box {i}, skipping crop.\")\n",
    "                continue\n",
    "\n",
    "            crop_pre = img_pre[y1:y2, x1:x2]\n",
    "            crop_post = img_post[y1:y2, x1:x2]\n",
    "\n",
    "            # Basic check for empty crops\n",
    "            if crop_pre.size == 0 or crop_post.size == 0:\n",
    "                 #print(f\"\\nWarning: Empty crop generated for {image_id} box {i}, skipping crop.\")\n",
    "                 continue\n",
    "\n",
    "            # Save crops\n",
    "            pre_crop_fname = f\"{image_id}_X_{i}_pre.jpg\"\n",
    "            post_crop_fname = f\"{image_id}_X_{i}_post.jpg\"\n",
    "            cv2.imwrite(os.path.join(crops_dir, pre_crop_fname), crop_pre)\n",
    "            cv2.imwrite(os.path.join(crops_dir, post_crop_fname), crop_post)\n",
    "\n",
    "            # Store filenames and the corresponding absolute bounding box\n",
    "            data[image_id]['pre_crops'].append(pre_crop_fname)\n",
    "            data[image_id]['post_crops'].append(post_crop_fname)\n",
    "            data[image_id]['bboxes'].append(boxes_abs[i]) # Store the absolute bbox coordinates\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {image_id}: {e}\")\n",
    "        # Clean up potentially partial data for this ID\n",
    "        if image_id in data:\n",
    "            del data[image_id]\n",
    "\n",
    "print(\"\\n[INFO] - Stage 1 finished. Saving crop data...\")\n",
    "with open(data_pkl_path, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "print(f\"[INFO] - Crop data saved to {data_pkl_path}\")\n",
    "\n",
    "# --- Classifier Definition ---\n",
    "CLASSIFIER_SZ = 128\n",
    "CLASSIFIER_MODEL = 'efficientvit_b0.r224_in1k'\n",
    "\n",
    "class DisasterClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DisasterClassifier, self).__init__()\n",
    "        # Load base model without classifier head\n",
    "        self.features = create_model(CLASSIFIER_MODEL, pretrained=False, num_classes=0) # Load weights later\n",
    "        # Get feature dimension dynamically (safer)\n",
    "        dummy_input = torch.randn(1, 3, CLASSIFIER_SZ, CLASSIFIER_SZ)\n",
    "        dummy_output = self.features(dummy_input)\n",
    "        feature_dim = dummy_output.shape[-1]\n",
    "\n",
    "        # Siamese distance calculation and final layer including NDVI\n",
    "        # Output dim of features * 2 (pre, post) + 1 (NDVI)? No, distance is 1 dim.\n",
    "        # Distance (1) + NDVI (1) = 2 inputs to final FC layer\n",
    "        self.fc = nn.Linear(1 + 1, 1) # 1 for distance, 1 for NDVI\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        # Ensure features are flattened if necessary or use appropriate distance for feature maps\n",
    "        # Assuming features are vectors [batch_size, feature_dim]\n",
    "        return torch.sqrt(torch.sum((x1 - x2)**2, dim=1, keepdim=True))\n",
    "\n",
    "    def forward(self, pre_image, post_image, ndvi_val):\n",
    "        pre_features = self.features(pre_image)\n",
    "        post_features = self.features(post_image)\n",
    "\n",
    "        # Calculate Euclidean distance between feature vectors\n",
    "        distance = self.euclidean_distance(pre_features, post_features)\n",
    "\n",
    "        # Concatenate distance and NDVI value\n",
    "        # Ensure ndvi_val has the correct shape [batch_size, 1]\n",
    "        combined_input = torch.cat([distance, ndvi_val], dim=1)\n",
    "\n",
    "        # Final classification layer\n",
    "        out = torch.sigmoid(self.fc(combined_input))\n",
    "        return out\n",
    "\n",
    "# --- Stage 2: Classification and Plotting ---\n",
    "print(\"\\n[INFO] - Starting Stage 2: Classification and Plotting...\")\n",
    "\n",
    "# Load NDVI data\n",
    "try:\n",
    "    test_df = pd.read_csv(ndvi_csv_path)\n",
    "    test_df.set_index('id', inplace=True) # Set index for easier lookup\n",
    "    print(f\"[INFO] - Loaded NDVI data from {ndvi_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NDVI data: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] - Using device: {device}\")\n",
    "\n",
    "classifier_model = DisasterClassifier()\n",
    "try:\n",
    "    classifier_model.load_state_dict(torch.load(classifier_model_path, map_location=device))\n",
    "    print(f\"[INFO] - Loaded classifier weights from {classifier_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading classifier model weights: {e}\")\n",
    "    # Attempting to load pretrained weights for the feature extractor part if main load failed\n",
    "    try:\n",
    "        print(\"[INFO] - Attempting to load TIMM pretrained weights for feature extractor...\")\n",
    "        classifier_model.features = create_model(CLASSIFIER_MODEL, pretrained=True, num_classes=0)\n",
    "        print(\"[INFO] - Loaded TIMM pretrained weights. Final layer remains untrained.\")\n",
    "    except Exception as e_timm:\n",
    "        print(f\"[ERROR] - Failed to load TIMM pretrained weights: {e_timm}. Cannot proceed.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "classifier_model = classifier_model.to(device)\n",
    "classifier_model.eval()\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(CLASSIFIER_SZ, CLASSIFIER_SZ),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], additional_targets={'image2': 'image'})\n",
    "# Load the data with bboxes\n",
    "print(f\"[INFO] - Loading data from {data_pkl_path}\")\n",
    "try:\n",
    "    with open(data_pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {data_pkl_path}. Please run Stage 1 first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "submission_ids = []\n",
    "submission_damage = []\n",
    "total_destroyed_count = 0\n",
    "\n",
    "# Plotting configuration\n",
    "destroyed_color = (0, 0, 255)  # Red in BGR\n",
    "nondamaged_color = (0, 255, 0) # Green in BGR\n",
    "alpha = 0.4  # Transparency factor for fill\n",
    "outline_thickness = 2\n",
    "\n",
    "# Process each image_id from the detected data\n",
    "for image_id in data.keys():\n",
    "    print(f\"[INFO] - Processing & Plotting: {image_id}\", flush=True)\n",
    "\n",
    "    # Retrieve NDVI value for the current image_id\n",
    "    try:\n",
    "        ndvi_mean = test_df.loc[image_id, 'NDVI_mean']\n",
    "        ndvi_tensor = torch.tensor([[ndvi_mean]], dtype=torch.float32).to(device) # Shape [1, 1]\n",
    "    except KeyError:\n",
    "        print(f\"Warning: NDVI value not found for {image_id}. Using 0. Skipping classification?\")\n",
    "        # Decide how to handle missing NDVI: skip image, use default, etc.\n",
    "        # For now, let's skip classification for this image_id if NDVI is missing.\n",
    "        # We still need dummy entries for the submission file for this ID.\n",
    "        print(f\"Skipping classification and plotting for {image_id} due to missing NDVI.\")\n",
    "        submission_ids.extend([f\"{image_id}_X_no_damage\", f\"{image_id}_X_minor_damage\", f\"{image_id}_X_major_damage\", f\"{image_id}_X_destroyed\"])\n",
    "        submission_damage.extend([0, 0, 0, 0]) # Append zeros if skipping\n",
    "        continue # Move to the next image_id\n",
    "\n",
    "    # Per-image results storage\n",
    "    results_for_image = [] # List to store (bbox, is_destroyed) tuples\n",
    "    damage_counts = {0: 0, 3: 0} # Store counts for no_damage (0) and destroyed (3)\n",
    "\n",
    "    # Classify each crop associated with this image_id\n",
    "    num_crops = len(data[image_id]['pre_crops'])\n",
    "    if num_crops == 0:\n",
    "        print(f\"Warning: No crops found for {image_id}, adding zeros to submission.\")\n",
    "    else:\n",
    "        for i in range(num_crops):\n",
    "            pre_crop_fname = data[image_id]['pre_crops'][i]\n",
    "            post_crop_fname = data[image_id]['post_crops'][i]\n",
    "            bbox = data[image_id]['bboxes'][i] # Retrieve the corresponding bbox\n",
    "\n",
    "            pre_crop_path = os.path.join(crops_dir, pre_crop_fname)\n",
    "            post_crop_path = os.path.join(crops_dir, post_crop_fname)\n",
    "\n",
    "            try:\n",
    "                pre_image = cv2.imread(pre_crop_path)\n",
    "                post_image = cv2.imread(post_crop_path)\n",
    "\n",
    "                if pre_image is None or post_image is None:\n",
    "                    print(f\"Warning: Could not read crop {pre_crop_fname} or {post_crop_fname}. Skipping patch.\")\n",
    "                    # How to handle this? Maybe count as non-damaged? Or skip entirely?\n",
    "                    # Skipping for now, it won't be plotted or counted.\n",
    "                    continue\n",
    "\n",
    "                # Preprocess images\n",
    "                pre_image_rgb = cv2.cvtColor(pre_image, cv2.COLOR_BGR2RGB)\n",
    "                post_image_rgb = cv2.cvtColor(post_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                transformed = valid_transform(image=pre_image_rgb, image2=post_image_rgb)\n",
    "                pre_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "                post_tensor = transformed['image2'].unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "                # Run classification inference\n",
    "                with torch.no_grad():\n",
    "                    output = classifier_model(pre_tensor, post_tensor, ndvi_tensor)\n",
    "                \n",
    "                # Check if destroyed (using 0.5 threshold as per original code)\n",
    "                is_destroyed = output.item() > 0.5\n",
    "\n",
    "                # Store result along with the bounding box\n",
    "                results_for_image.append((bbox, is_destroyed))\n",
    "\n",
    "                # Update counts\n",
    "                if is_destroyed:\n",
    "                    damage_counts[3] += 1\n",
    "                    total_destroyed_count += 1\n",
    "                else:\n",
    "                    damage_counts[0] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing crop pair {pre_crop_fname}/{post_crop_fname}: {e}\")\n",
    "                continue # Skip this problematic crop pair\n",
    "\n",
    "\n",
    "    # --- Plotting for the current image_id ---\n",
    "    # Load the original post-disaster image\n",
    "    post_img_path = os.path.join(test_dir, f\"{image_id}_post_disaster.{EXT}\")\n",
    "    post_img = cv2.imread(post_img_path)\n",
    "\n",
    "    if post_img is None:\n",
    "        print(f\"Warning: Could not read post image {post_img_path} for annotation.\")\n",
    "    elif not results_for_image:\n",
    "         print(f\"Info: No valid classification results for {image_id}, skipping annotation.\")\n",
    "    else:\n",
    "        overlay = post_img.copy() # Create a copy for drawing transparent fills\n",
    "\n",
    "        # Draw filled rectangles on the overlay\n",
    "        for bbox, is_destroyed in results_for_image:\n",
    "            x1, y1, x2, y2 = map(int, bbox) # Ensure integer coordinates\n",
    "            color = destroyed_color if is_destroyed else nondamaged_color\n",
    "            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1) # -1 for filled\n",
    "\n",
    "        # Blend the overlay with the original image\n",
    "        cv2.addWeighted(overlay, alpha, post_img, 1 - alpha, 0, post_img)\n",
    "\n",
    "        # Draw outlines on the blended image (optional, for better visibility)\n",
    "        for bbox, is_destroyed in results_for_image:\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            color = destroyed_color if is_destroyed else nondamaged_color\n",
    "            cv2.rectangle(post_img, (x1, y1), (x2, y2), color, outline_thickness)\n",
    "\n",
    "        # Save the annotated image\n",
    "        annotated_img_path = os.path.join(annotated_dir, f\"{image_id}_post_disaster_annotated.png\")\n",
    "        cv2.imwrite(annotated_img_path, post_img)\n",
    "        # print(f\"Saved annotated image to {annotated_img_path}\") # Can uncomment for more verbose output\n",
    "\n",
    "    # --- Prepare submission data for this image_id ---\n",
    "    # Append counts based on the classification results for this image\n",
    "    submission_ids.append(f\"{image_id}_X_no_damage\")\n",
    "    submission_damage.append(damage_counts[0])\n",
    "    submission_ids.append(f\"{image_id}_X_minor_damage\")\n",
    "    submission_damage.append(0) # Explicitly 0 based on problem description\n",
    "    submission_ids.append(f\"{image_id}_X_major_damage\")\n",
    "    submission_damage.append(0) # Explicitly 0 based on problem description\n",
    "    submission_ids.append(f\"{image_id}_X_destroyed\")\n",
    "    submission_damage.append(damage_counts[3])\n",
    "\n",
    "\n",
    "# --- Final Submission CSV ---\n",
    "print(\"\\n[INFO] - Generating final submission file...\")\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'damage': submission_damage})\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "print(f\"[INFO] - Submission file saved to {submission_csv_path}\")\n",
    "print(f\"[INFO] - Total 'destroyed' count across all images: {total_destroyed_count}\")\n",
    "print(\"[INFO] - Script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6595070,
     "sourceId": 10651517,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
